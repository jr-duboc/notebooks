{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic definitions\n",
    "\n",
    "## Big O (worst case)\n",
    "\n",
    "$f(n) = O(g(n))$ means that there is a variable $c$ such that, for all sufficiently big $n$, we always have  $f(n) \\leq c.g(n)$.  \n",
    "We say that $c.g(n)$ is an _upper bound_ of $f(n)$. \n",
    "\n",
    "A function can have multiple upper bounds, for instance $n\\times2 = O(n)$ because $n\\times2$ is always bigger than $n$ when $n > 0$, but we can also say that $n\\times3 = O(n)$ for the same reason. In fact, we can also say $n\\times3 = O(n\\times2)$. Most of the time we'll say that $n \\times x = O(n)$, though. \n",
    "All we're saying here is that, when looking at the function $f(n)=n$, for $n = 1000$, $f(n)$ is smaller than 2000, 3000, or even 10,000! For a trivial function like that it seems a bit silly, but a function representing an algorithm's run time can be a lot more complex than that, so we need a model that tells us the worst possible case we can encounter as $n$ gets bigger.\n",
    "\n",
    "This is the most useful tool to analyse algorithms in practice because, of course, we want our algorithms to be as fast as possible. Therefore, we only really care about the worst case scenario, the longest time an algorithms can take. Anything faster than that is just gravy.  \n",
    "Knowing _exactly_ how an algorithm behaves for any and all types and size of inputs, while interesting, doesn't really help to improve it all that much in most cases, and that's where big $O$ notation is so useful when designing algorithms; it provides a neat shortcut to guarantee a maximum runtime.\n",
    "\n",
    "## Big Omega ($\\Omega$) (best case)\n",
    "\n",
    "$f(n) = \\Omega(g(n))$ is the counterpart of $O$. It means that there's a variable $c$ that defines a _lower bound_  for $g(n)$.  \n",
    "In other words, there's a variable $c$ such that $f(n) \\geq c.g(n)$.\n",
    "This is not that useful in practice, because it just tells us about the very best possible scenario, and in the real world that scenario rarely happens anyway.\n",
    "\n",
    "## Theta ($\\Theta$) (average case)\n",
    "\n",
    "$f(n) = \\Theta(g(n))$ provides us with a \"ballbark\" function that will tell us how our algorithm will behave on average. \n",
    "We say that there exists two variables $c_1$ and $c_2$ such as, on the one hand, $f(n) \\leq c_1\\times g(n)$, and on the other hand $f(n) \\geq c_2\\times g(n)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance families\n",
    "\n",
    "## Constant performance\n",
    "\n",
    "## Logarithmic performance\n",
    "\n",
    "## Sublinear performance\n",
    "\n",
    "## Linear performance\n",
    "\n",
    "## $n log(n)$ performance\n",
    "\n",
    "## Quadratic performance\n",
    "\n",
    "## Exponential performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
