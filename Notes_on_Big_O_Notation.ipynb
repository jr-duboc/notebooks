{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic definitions\n",
    "\n",
    "## Big O (worst case)\n",
    "\n",
    "$f(n) = O(g(n))$ means that there is a variable $c$ such that, for all sufficiently big $n$, we always have  $f(n) \\leq c.g(n)$.  \n",
    "We say that $c.g(n)$ is an _upper bound_ of $f(n)$. \n",
    "\n",
    "A function can have multiple upper bounds, for instance $n\\times2 = O(n)$ because $n\\times2$ is always bigger than $n$ when $n > 0$, but we can also say that $n\\times3 = O(n)$ for the same reason. In fact, we can also say $n\\times3 = O(n\\times2)$. Most of the time we'll say that $n \\times x = O(n)$, though. \n",
    "All we're saying here is that, when looking at the function $f(n)=n$, for $n = 1000$, $f(n)$ is smaller than 2000, 3000, or even 10,000! For a trivial function like that it seems a bit silly, but a function representing an algorithm's run time can be a lot more complex than that, so we need a model that tells us the worst possible case we can encounter as $n$ gets bigger.\n",
    "\n",
    "This is the most useful tool to analyse algorithms in practice because, of course, we want our algorithms to be as fast as possible. Therefore, we only really care about the worst case scenario, the longest time an algorithms can take. Anything faster than that is just gravy.  \n",
    "Knowing _exactly_ how an algorithm behaves for any and all types and size of inputs, while interesting, doesn't really help to improve it all that much in most cases, and that's where big $O$ notation is so useful when designing algorithms; it provides a neat shortcut to guarantee a maximum runtime.\n",
    "\n",
    "## Big Omega ($\\Omega$) (best case)\n",
    "\n",
    "$f(n) = \\Omega(g(n))$ is the counterpart of $O$. It means that there's a variable $c$ that defines a _lower bound_  for $g(n)$.  \n",
    "In other words, there's a variable $c$ such that $f(n) \\geq c.g(n)$.\n",
    "This is not that useful in practice, because it just tells us about the very best possible scenario, and in the real world that scenario rarely happens anyway.\n",
    "\n",
    "## Theta ($\\Theta$) (average case)\n",
    "\n",
    "$f(n) = \\Theta(g(n))$ provides us with a \"ballbark\" function that will tell us how our algorithm will behave on average. \n",
    "We say that there exists two variables $c_1$ and $c_2$ such as, on the one hand, $f(n) \\leq c_1\\times g(n)$, and on the other hand, $f(n) \\geq c_2\\times g(n)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance families\n",
    "\n",
    "Algorithm analysis enables us to compare performance and classify algorithm according to their relative worst-case runtime.\n",
    "Here are the main families of algorithm performance, based on the big O function for each.\n",
    "They are ordered from slowest to fastest, as in: $n! \\gg 2^n \\gg n^3 \\gg n^2 \\gg n\\log n \\gg n \\gg \\log n \\gg 1$\n",
    "\n",
    "## Constant performance\n",
    "\n",
    "An algorithms of constant performance always takes the same time to perform regardless of input, given the same underlying hardware. This rarely ever happens in practice, outside of very trivial operations.  \n",
    "We write the performance of these algorithms as $O(1)$.\n",
    "\n",
    "## Logarithmic performance\n",
    "\n",
    "Logarithmic algorithms are extremely efficient, because they tend to divide the problem into sub-problems (divid and conquer), and hone in on a solution. As a result they reduce the size of the problem by half each time they iterate.  \n",
    "We write the performance for these as $O(log(n))$.\n",
    "\n",
    "## Sublinear performance\n",
    "\n",
    "Performance for these algorithms is in the form $O(n^d)$ (with $d > 1$); they are less efficient than logarithmic algorithms, and can get extremely slow for very big $n$.\n",
    "\n",
    "## Linear performance\n",
    "\n",
    "These are modelled on a simple function $O(n)$, meaning we run an atomic operation for every element provided as an input.\n",
    "\n",
    "## Superlinear performance\n",
    "\n",
    "The Quicksort and Mergesort algorithms belong to this category.  The function modelling these algorithms' growth is written in the form $O(n\\lg n)$. This function goes up just a little faster than a linear equation.\n",
    "\n",
    "## Quadratic and Cubic performance\n",
    "\n",
    "We start to get into much slower algorithms there, with $O(n^2)$ (quadratic) and $O(n^3)$ (cubic).\n",
    "Search algorithms such as insertion and selection sort examine all pairs or triplets of items in an input containing $n$ elements. The quadratic and cubic functions represent the performance cost of such strategies.\n",
    "\n",
    "## Exponential and Factorial performance\n",
    "\n",
    "This is what happens when we use an algorithm to examine all subset of $n$ items provided as an input: $O(c^n)$. This grows very slow and useless incredibly fast.\n",
    "A factorial algorithm ($O(n!)$) generates all permutations or ordering of n items. This is about as slow as it gets, and completely useless for sufficiently big $n$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
